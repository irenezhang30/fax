{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu\n"
     ]
    }
   ],
   "source": [
    "from absl.testing import absltest\n",
    "from absl.testing import parameterized\n",
    "\n",
    "import copy\n",
    "import collections \n",
    "\n",
    "import jax.test_util\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from jax import lax, vjp, custom_vjp, grad, jacrev, jacfwd, random, tree_util, jacfwd\n",
    "from jax.experimental import optimizers\n",
    "from jax.scipy.special import logsumexp\n",
    "from jax.experimental.stax import softmax\n",
    "from jax.config import config\n",
    "from jax.random import bernoulli\n",
    "from jax.numpy.linalg import norm\n",
    "\n",
    "from fax import converge, test_util\n",
    "from fax.constrained import implicit_ecp\n",
    "from fax.loop import fixed_point_iteration\n",
    "from fax.implicit.twophase import make_adjoint_fixed_point_iteration\n",
    "from fax.implicit.twophase import make_forward_fixed_point_iteration\n",
    "\n",
    "# check device\n",
    "from jax.lib import xla_bridge\n",
    "print(xla_bridge.get_backend().platform)\n",
    "\n",
    "FixedPointSolution = collections.namedtuple(\n",
    "    \"FixedPointSolution\",\n",
    "    \"value converged iterations previous_value\"\n",
    ")\n",
    "\n",
    "#just in time compilation\n",
    "from jax import jit\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "config.update('jax_disable_jit', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_transition = jnp.array([[[0.9, 0.1], [0.2, 0.8]],\n",
    "                             [[0.8, 0.2], [0.99, 0.01]]])\n",
    "temperature = 1e-2\n",
    "\n",
    "true_discount = 0.9\n",
    "\n",
    "traj_len = 75\n",
    "\n",
    "initial_distribution = jnp.ones(2) / 2\n",
    "\n",
    "policy_expert = jnp.array(([[0.45, 0.55],\n",
    "                            [0.55,  0.45]]))\n",
    "key = random.PRNGKey(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_key():\n",
    "    global key\n",
    "    mykey, subkey = random.split(key)\n",
    "    key = subkey\n",
    "\n",
    "\n",
    "def roll_out(last_state, last_action, p, model):\n",
    "    global key\n",
    "    get_new_key()\n",
    "    s = bernoulli(key, p=p[last_action][last_state][0]).astype(int)\n",
    "    get_new_key()\n",
    "    a = bernoulli(key, p=model[s][0]).astype(int)\n",
    "    return (s, a)\n",
    "\n",
    "\n",
    "def sample_trajectory(policy):\n",
    "    get_new_key()\n",
    "    s = bernoulli(key, p=initial_distribution[0]).astype(int)\n",
    "    get_new_key()\n",
    "    a = bernoulli(key, p=policy[s][0]).astype(int)\n",
    "    traj = []\n",
    "    traj.append((s, a))\n",
    "    for i in range(traj_len-1):\n",
    "        s, a = roll_out(s, a, true_transition, policy)\n",
    "        traj.append((s, a))\n",
    "    return jnp.array(copy.deepcopy(traj))\n",
    "\n",
    "#ratio_loss\n",
    "def L(theta, w, traj_model, traj_expert):\n",
    "    del theta\n",
    "    discriminator = softmax((1. / temperature) * w)    \n",
    "    loss = 0\n",
    "    for i in range(traj_len):\n",
    "        s_expert, a_expert = traj_expert[i]\n",
    "        s_model, a_model = traj_model[i]\n",
    "        loss += - jnp.log(discriminator[s_expert][a_expert]) - jnp.log(1 - discriminator[s_model][a_model])\n",
    "    return loss/traj_len\n",
    "\n",
    "F = grad(L, (1))\n",
    "\n",
    "# generator loss\n",
    "def J(theta, w, traj_model):\n",
    "    del theta\n",
    "    discriminator = softmax((1. / temperature) * w)\n",
    "    loss = 0\n",
    "    for i in range(traj_len):\n",
    "        s_model, a_model = traj_model[i]\n",
    "        loss += jnp.log(discriminator[s_model][a_model])\n",
    "    return loss / traj_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = jnp.array([[1.0, 1.1],[1.02,0.99]])\n",
    "# theta = jnp.array([[1.1, 1.0],[0.99,1.0]])\n",
    "\n",
    "w = jnp.ones((2,2))\n",
    "theta = jnp.ones((2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find constraint solution (w*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constraints_solver(theta, w, max_iter=100, threshold = 1e-3):\n",
    "    \n",
    "    #initialize optimizer\n",
    "    opt_init, opt_update, get_params = optimizers.adam(step_size=0.01)\n",
    "    opt_state = opt_init(w)\n",
    "    prev = w\n",
    "    \n",
    "    policy_model = softmax((1. / temperature) * theta)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        traj_model = sample_trajectory(policy_model)\n",
    "        traj_expert = sample_trajectory(policy_expert)\n",
    "        \n",
    "        ratio_grad = F(theta, w, traj_model, traj_expert)\n",
    "        opt_state = opt_update(i, ratio_grad, opt_state)\n",
    "        w = get_params(opt_state)\n",
    "    \n",
    "        #check threshold\n",
    "        if i > 0 and jnp.max(jnp.abs(w - prev)) <= threshold:\n",
    "            return FixedPointSolution(\n",
    "                value=w,\n",
    "                converged=True,\n",
    "                iterations=i,\n",
    "                previous_value=prev,\n",
    "            )\n",
    "        if i < max_iter - 1:\n",
    "            prev = w\n",
    "    \n",
    "    return FixedPointSolution(\n",
    "        value=w,\n",
    "        converged=False,\n",
    "        iterations=max_iter,\n",
    "        previous_value=prev,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FixedPointSolution(value=DeviceArray([[1.0063526 , 0.9936474 ],\n",
       "             [0.99571118, 1.00428882]], dtype=float64), converged=True, iterations=8, previous_value=DeviceArray([[1.00623082, 0.99376918],\n",
       "             [0.9957778 , 1.0042222 ]], dtype=float64))"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_solution = constraints_solver(theta, w)\n",
    "forward_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = softmax((1. / temperature) * theta)\n",
    "traj_model = sample_trajectory(policy_model)\n",
    "traj_expert = sample_trajectory(policy_expert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find dJ/dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-2.66666667,  2.66666667],\n",
       "             [-0.66666667,  0.66666667]], dtype=float64)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = jnp.ones((2,2))\n",
    "grad(J, (1))(theta, w, traj_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-2.66666667,  2.66666667],\n",
       "             [-0.66666667,  0.66666667]], dtype=float64)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dJ_dw  = grad(J, argnums=1)(theta, w, traj_model)\n",
    "dJ_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find dJ/dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_policy(theta, s, a):\n",
    "    policy_model = softmax((1. / temperature) * theta)\n",
    "    return jnp.log(policy_model[s][a])\n",
    "\n",
    "policy_grad = jax.grad(get_log_policy, (0))\n",
    "\n",
    "\n",
    "def discounted_reward(t, rewards, gamma = 0.9):\n",
    "    discounted = [ gamma**(i-t) * rewards[i] for i in range(t, len(rewards))]\n",
    "    G = jnp.array(discounted).sum()\n",
    "    return G\n",
    "\n",
    "\n",
    "def reinforce(theta, traj_model, rewards):\n",
    "    estimator = 0\n",
    "    for t in range(len(theta)):\n",
    "        #grad of log policy\n",
    "        s_model, a_model = traj_model[t]\n",
    "        grad_log_policy = policy_grad(theta, s_model, a_model) \n",
    "        reward = discounted_reward(t, rewards)\n",
    "        estimator += grad_log_policy * reward\n",
    "    return estimator / traj_len\n",
    "\n",
    "\n",
    "def get_dJ_dtheta(theta, w, traj_model):\n",
    "    discriminator = softmax((1. / temperature) * w)\n",
    "    rewards = []\n",
    "    for i in range(len(traj_model)):\n",
    "        s_model,a_model = traj_model[i]\n",
    "        rewards.append(jnp.log(discriminator[s_model][a_model]))\n",
    "    \n",
    "    return reinforce(theta, traj_model, rewards)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 3.46563362, -3.46563362],\n",
       "             [-3.46564385,  3.46564385]], dtype=float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dJ_dtheta = get_dJ_dtheta(theta, w, traj_model)\n",
    "dJ_dtheta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find dF/dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dis_temp(w, s_model, a_model):\n",
    "    discriminator = softmax((1. / temperature) * w)\n",
    "    return jnp.log(1-discriminator[s_model][a_model])\n",
    "\n",
    "grad_dis_w = jax.grad(dis_temp)\n",
    "\n",
    "def get_dF_dtheta(theta, w, traj_model):\n",
    "    rewards = []\n",
    "    for i in range(len(traj_model)):\n",
    "        s_model,a_model = traj_model[i]\n",
    "        rewards.append(-grad_dis_w(w, s_model, a_model))\n",
    "    return reinforce(theta, traj_model, rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[0., 0.],\n",
       "             [0., 0.]], dtype=float64)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dF_dtheta = get_dF_dtheta(theta, w, traj_model)\n",
    "dF_dtheta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find dF/dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dF_dw = jax.hessian(L, argnums=1)\n",
    "dF_dw = get_dF_dw(theta, w, traj_model, traj_expert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 2.75730589e+12,  2.75730589e+12,  0.00000000e+00,\n",
       "               0.00000000e+00],\n",
       "             [ 2.75730589e+12,  2.75730589e+12,  0.00000000e+00,\n",
       "               0.00000000e+00],\n",
       "             [ 0.00000000e+00,  0.00000000e+00, -1.09951163e+12,\n",
       "              -1.09951163e+12],\n",
       "             [-0.00000000e+00, -0.00000000e+00, -1.09951163e+12,\n",
       "              -1.09951163e+12]], dtype=float64)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.linalg.inv(dF_dw.reshape((4,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find implicit gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dphi_dtheta = (jnp.linalg.inv(dF_dw.reshape((4,4))).dot(dF_dtheta.flatten()))\n",
    "dphi_dtheta = dphi_dtheta.reshape(2,2)\n",
    "implicit_grad = dJ_dw.dot(dphi_dtheta) + dJ_dtheta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implicit_diff(theta, w, max_iter=100, threshold = 1e-3):\n",
    "    \n",
    "    #initialize optimizer\n",
    "    opt_init, opt_update, get_params = optimizers.adam(step_size=0.001)\n",
    "    opt_state = opt_init(theta)\n",
    "    prev = theta\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        \n",
    "        prev_theta = theta\n",
    "        print (i)\n",
    "        # get converged discriminator logits\n",
    "        forward_solution = constraints_solver(theta, w)\n",
    "        assert forward_solution.converged == True \n",
    "        w = forward_solution.value\n",
    "        print (\"w\", w)\n",
    "        \n",
    "        policy_model = softmax((1. / temperature) * theta)\n",
    "        traj_model = sample_trajectory(policy_model)\n",
    "        traj_expert = sample_trajectory(policy_expert)\n",
    "        \n",
    "        dJ_dtheta = get_dJ_dtheta(theta, w, traj_model)\n",
    "        print (\"dJ_dtheta\", dJ_dtheta)\n",
    "        dJ_dw  = grad(J, (1))(theta, w, traj_model)\n",
    "        print (\"dJ_dw\",  dJ_dw)\n",
    "        dF_dtheta = get_dF_dtheta(theta, w, traj_model)\n",
    "        print (\"dF_dtheta\", dF_dtheta)\n",
    "        dF_dw = get_dF_dw(theta, w, traj_model, traj_expert)\n",
    "        print (\"dF_dw\", dF_dw.reshape((4,4)))\n",
    "        print (\"inverse: \", jnp.linalg.inv(dF_dw.reshape((4,4))))\n",
    "        \n",
    "        dphi_dtheta = (jnp.linalg.pinv(dF_dw.reshape((4,4))).dot(dF_dtheta.flatten()))\n",
    "        dphi_dtheta = dphi_dtheta.reshape(2,2)\n",
    "        implicit_grads = dJ_dw.dot(dphi_dtheta) + dJ_dtheta\n",
    "        print (\"implicit_grads\", implicit_grads)\n",
    "        \n",
    "        opt_state = opt_update(i, implicit_grads, opt_state)\n",
    "        theta = get_params(opt_state)\n",
    "        policy_model = softmax((1. / temperature) * theta)\n",
    "        print (\"theta\", theta)\n",
    "        print (\"policy\", policy_model)\n",
    "        print (\"\")\n",
    "        #check threshold\n",
    "        if i > 0 and jnp.max(jnp.abs(theta - prev)) <= threshold:\n",
    "            return theta\n",
    "        if i < max_iter - 1:\n",
    "            prev = theta\n",
    "    \n",
    "    print (\"not converged\")\n",
    "    return theta\n",
    "    \n",
    "\n",
    "# use LINALG.SOLVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# w = jnp.array([[0.99, 0.998],[1.01, 1.1]])\n",
    "# theta = jnp.array([[0.99, 1.01],[1.0, 0.9]])\n",
    "\n",
    "#collect samples \n",
    "\n",
    "w = jnp.ones((2,2))\n",
    "theta = jnp.ones((2,2))\n",
    "\n",
    "implicit_diff(theta, w,threshold = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = jnp.ones((2,2))\n",
    "theta = jnp.ones((2,2))\n",
    "\n",
    "initial_values = (theta, w)\n",
    "opt_init, opt_update, get_params = optimizers.adam(step_size=0.001)\n",
    "\n",
    "x0, init_params = initial_values\n",
    "opt_state = opt_init(init_params)\n",
    "\n",
    "def update(i, values):\n",
    "    w, opt_state = values\n",
    "    theta = get_params(opt_state)\n",
    "\n",
    "    # get converged discriminator logits\n",
    "    forward_solution = constraints_solver(theta, w)\n",
    "    assert forward_solution.converged == True \n",
    "    w = forward_solution.value\n",
    "    \n",
    "    traj_model = sample_trajectory(policy_model)\n",
    "    dJ_dtheta = get_dJ_dtheta(theta, w, traj_model)\n",
    "    dJ_dw  = grad(J, (1))(theta, w, traj_model)\n",
    "    dF_dtheta = get_dF_dtheta(theta, w, traj_model)\n",
    "    dF_dw = get_dF_dw(theta, w, traj_model, traj_expert)\n",
    "    \n",
    "    dphi_dtheta = (jnp.linalg.inv(dF_dw.reshape((4,4))).dot(dF_dtheta.flatten()))\n",
    "    dphi_dtheta = dphi_dtheta.reshape(2,2)\n",
    "    implicit_grads = dJ_dw.dot(dphi_dtheta) + dJ_dtheta\n",
    "    opt_state = opt_update(i, implicit_grads, opt_state)\n",
    "\n",
    "    return forward_solution.value, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convergence_test(x_new, x_old):\n",
    "    min_type = converge.tree_smallest_float_dtype(x_new)\n",
    "    rtol, atol = converge.adjust_tol_for_dtype(1e-10, 1e-10, min_type)\n",
    "    return converge.max_diff_test(x_new, x_old, rtol, atol)\n",
    "\n",
    "def _convergence_test(new_state, old_state):\n",
    "    x_new, params_new = new_state[0], get_params(new_state[1])\n",
    "    x_old, params_old = old_state[0], get_params(old_state[1])\n",
    "    return convergence_test((x_new, params_new), (x_old, params_old))\n",
    "\n",
    "solution = fixed_point_iteration(init_x=(x0, opt_state),\n",
    "                                  func=update,\n",
    "                                  convergence_test=_convergence_test,\n",
    "                                  max_iter=50,\n",
    "                                  batched_iter_size=1,\n",
    "                                  unroll=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
